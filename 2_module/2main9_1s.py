#2main9_1s.py
#Парсинг сайтов
#pip install requests - позволит обращаться по какому-либо url
#pip install beautifulsoup4 - стягивает всю разметку в коде
# и далее можем найти ту инфу,кот.надо скопировать

import requests
#если не подсвечивается красным,значит библиотеки установлены
from bs4 import BeautifulSoup as BS

headers = {
    'Accept' : '*/*', #пишем что мы принимаем все файлы и все док-ты
    'User-Agent' : 'Mozilla/5.0 (platform; rv:geckoversion) Gecko/geckotrail Firefox/firefoxversion',
#нужно указать си-му с которой пришел польз-ль(нужно програмное название
# (найти в гугле можно.Прописать header user agent и найти там
# пример Mozilla/5.0 (platform; rv:geckoversion) Gecko/geckotrail Firefox/firefoxversion)))
}

#теперь надо получить запрос с сайта,чтобы получить HTML код

url = 'https://itproger.com' #иногда стоят защитные проги
#обращаемся к библиотеке и к класу .Session()(создаем сессию)

# url = 'https://itproger.com/sdf' #можно проверить обратившись к несуществующему адресу(будет ошибка)

#url = 'https://itprogersdf.com' #также можно обратиться к вообще не существующему адрессу(будет ошибка)
#чтобы этого не было можно поместить в try-exept(обработчик исключений)

session = requests.Session()
#теперь обращаемся к session,ф-ии get и к (url)
#скорее всего будет выдавать ошибку,т.к. это бот,надо написать заголовки в get()headers=headers
#Но сначало их нужно создать.

try:
    res = session.get(url,headers=headers)

#теперь надо проверить удалось ли достучатся до сайта:
#обращаемся к res,и ф-ии .status_code(програмный ответ с сервера нашего запроса.Хуй знает что это значит)
#если ответ 200,то это значит что мы корректно подключились к серверу

    if res.status_code == 200:
        #теперь надо прочитать весь html-код и вывести его на экран
        # print("Ok")
        soup = BS(res.content,'html.parser') #обратились к BS,дали в него пар-тр res и ф-ии .content,
#дальше дали 2й пар-тр('html.parser')---указали как будем работать с этими данными(будем парсить или копировать)
#закинули это все в пер-ую и теперь просто хотим вывести print(soup)
        #print(soup) #выбивает весь HTML-code
#чтобы выбрать то,что нам нужно,нужно найти за что можно зацепиться.Тоесть найти такой <div>,который отвечает
#курс уроки python для начинающих.Будем искать по тегу course
        divs = soup.find_all('div',attrs={'class':'course'}) #обратились к soup и к ф-ии .find-ищет 1 блок
#если хотим найти все блоки:.find_all ---<a href="courses/web"> html - курса
# (передаем сюда атрибуты 'div'- название тега,attrs={'class':'course'} - ищем такой класс с названием 'course')
#     print(divs)

#теперь надо найти название курсов(за это отвечает тег <span class="title_course bigger".Делаем это перебором
        for el in divs:
            title = el.find('span',attrs = {'title_course'}) #создали перебор,и в .find(пишем название тега 'span'
#и тег 'title_course'
            #print(title) #при таком выводе пишется все вместе с остальными тегами.Чтоб такого не было надо
#прописать print(title.stripped_strings)- св-во позволяет из обьекта 'span' получить только текст.И если он в разных
#тегах он будет разбит на разные э-ты.Поэтому надо прописать list-создаем список,и вытягиваем 1й эл-нт[0]

# теперь чтобы вывести ссылку на сам курс,нужно скопировать ссылку на этот курс <a href="course/angular">
#создаем новую переменную,указываю,что нахожу ссылку 'a',и получаем значение атрибута ['href'] - наша ссылка
            href = el.find('a')['href']
            result = f"{list(title.stripped_strings)[0]}: {url}/{href}"   #плюс указываем ссылку в форматированной строке
#и плюс url {url} и слеш / {href}
            print(result)


    else:
        print("Ошибка")
except Exception: #проверяем базовый класс Exeption
    print('Ошибка в самом url-адрессе')
